ANN 1

import numpy as np
import matplotlib.pyplot as plt

def plot_activation_function(function, title):
    t = np.linspace(-5, 5)
    plt.figure(figsize=(8, 4))
    plt.plot(t, function(t))
    plt.title(title)
    plt.show()

# Binary Sigmoid
plot_activation_function(lambda t: 1 / (1 + np.exp(-t)), 'Binary Sigmoid Activation Function')

# Bipolar Sigmoid (Hyperbolic Tan)
plot_activation_function(np.tanh, 'Hyperbolic Tan Activation Function')

# ReLU (Rectified Linear Unit)
plot_activation_function(lambda t: np.maximum(t, 0), 'Rectified Linear Unit Activation Function')

# Leaky ReLU
plot_activation_function(lambda t: np.where(t >= 0, t, 0.01 * t), 'Leaky ReLU Activation Function')

# ELU (Exponential Linear Unit)
plot_activation_function(lambda t: np.where(t < 0, 0.5 * (np.exp(t) - 1), t), 'ELU Activation Function')

# Softmax
plot_activation_function(lambda t: np.exp(t) / np.sum(np.exp(t)), 'Softmax Activation Function')
