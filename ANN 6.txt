ANN 6

import numpy as np
# Define the sigmoid activation function
def sigmoid(x):
return 1 / (1 + np.exp(-x))
# Define the derivative of the sigmoid function
def sigmoid_derivative(x):
return x * (1 - x)
# Define the training data
X = np.array([[0, 0, 1],
[0, 1, 1],
[1, 0, 1],
[1, 1, 1]])
y = np.array([[0],
[1],
[1],
[0]])
# Define the hyperparameters
epochs = 10000
learning_rate = 0.1
# Initialize the weights randomly
np.random.seed(1)
weights_0 = 2 * np.random.random((3, 4)) - 1
weights_1 = 2 * np.random.random((4, 1)) - 1
# Train the neural network using forward propagation
for i in range(epochs):
# Forward propagation
layer_0 = X
layer_1 = sigmoid(np.dot(layer_0, weights_0))
layer_2 = sigmoid(np.dot(layer_1, weights_1))
# Calculate the error and delta for layer 2
layer_2_error = y - layer_2
layer_2_delta = layer_2_error * sigmoid_derivative(layer_2)
# Calculate the error and delta for layer 1
layer_1_error = layer_2_delta.dot(weights_1.T)
layer_1_delta = layer_1_error * sigmoid_derivative(layer_1)
# Update the weights
weights_1 += learning_rate * layer_1.T.dot(layer_2_delta)
weights_0 += learning_rate * layer_0.T.dot(layer_1_delta)
# Test the neural network
test_data = np.array([[0, 0, 1],
[0, 1, 1],
[1, 0, 1],
[1, 1, 1]])
predicted_output = sigmoid(np.dot(sigmoid(np.dot(test_data, weights_0)), weights_1)).round()
print(predicted_output)
[[0.]
 [1.]
 [1.]
 [0.]]


# Define the sigmoid activation function
def sigmoid(x):
return 1 / (1 + np.exp(-x))
# Define the derivative of the sigmoid function
def sigmoid_derivative(x):
return x * (1 - x)
# Define the training data
X = np.array([[0, 0],
[0, 1],
[1, 0],
[1, 1]])
y = np.array([[0],
[1],
[1],
[0]])
# Define the hyperparameters
epochs = 10000
learning_rate = 0.1
# Initialize the weights randomly
np.random.seed(1)
weights_0 = 2 * np.random.random((2, 4)) - 1
weights_1 = 2 * np.random.random((4, 1)) - 1
# Train the neural network using back propagation
for i in range(epochs):
# Forward propagation
layer_0 = X
layer_1 = sigmoid(np.dot(layer_0, weights_0))
layer_2 = sigmoid(np.dot(layer_1, weights_1))
# Calculate the error for layer 2
layer_2_error = y - layer_2
# Back propagation for layer 2
layer_2_delta = layer_2_error * sigmoid_derivative(layer_2)
layer_1_error = layer_2_delta.dot(weights_1.T)
# Back propagation for layer 1
layer_1_delta = layer_1_error * sigmoid_derivative(layer_1)
# Update the weights
weights_1 += learning_rate * layer_1.T.dot(layer_2_delta)
weights_0 += learning_rate * layer_0.T.dot(layer_1_delta)
# Test the neural network
test_data = np.array([[0, 0],
[0, 1],
[1, 0],
[1, 1]])
predicted_output = sigmoid(np.dot(sigmoid(np.dot(test_data, weights_0)), weights_1)).round()
print(predicted_output)
[[0.]
 [1.]
 [1.]
 [0.]]