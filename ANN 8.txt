ANN 8


# Backpropagation feed-forward network
import numpy as np
import pandas as pd
# Define the sigmoid activation function
def sigmoid(x):
return 1 / (1 + np.exp(-x))
# Define the derivative of the sigmoid activation function
def sigmoid_derivative(x):
return x * (1 - x)
# Define the neural network class
class NeuralNetwork:
def __init__(self, inputs, hidden, outputs):
# Initialize the weights for the input and hidden layers
self.weights_ih = np.random.rand(inputs, hidden)
# Initialize the weights for the hidden and output layers
self.weights_ho = np.random.rand(hidden, outputs)
def feedforward(self, inputs):
# Calculate the dot product of the inputs and the weights for the input and hidden layers
hidden_inputs = np.dot(inputs, self.weights_ih)
# Apply the sigmoid activation function to the hidden layer
hidden_outputs = sigmoid(hidden_inputs)
# Calculate the dot product of the hidden layer and the weights for the hidden and output layers
output_inputs = np.dot(hidden_outputs, self.weights_ho)
# Apply the sigmoid activation function to the output layer
output_outputs = sigmoid(output_inputs)
return output_outputs
def train(self, inputs, targets, learning_rate):
# Feed forward the inputs through the neural network
hidden_inputs = np.dot(inputs, self.weights_ih)
hidden_outputs = sigmoid(hidden_inputs)
output_inputs = np.dot(hidden_outputs, self.weights_ho)
output_outputs = sigmoid(output_inputs)
# Calculate the error between the target outputs and the actual outputs
output_error = targets - output_outputs
# Calculate the derivative of the sigmoid activation function for the output layer
output_derivative = sigmoid_derivative(output_outputs)
# Calculate the error for the hidden layer
hidden_error = np.dot(output_error, self.weights_ho.T)
# Calculate the derivative of the sigmoid activation function for the hidden layer
hidden_derivative = sigmoid_derivative(hidden_outputs)
# Update the weights for the hidden and output layers
self.weights_ho += learning_rate * np.dot(hidden_outputs.T, output_error * output_derivative)
# Update the weights for the input and hidden layers
self.weights_ih += learning_rate * np.dot(inputs.T, hidden_error * hidden_derivative)
nn = NeuralNetwork(2, 4, 1)
# Define the XOR inputs and targets
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
targets = np.array([[0], [1], [1], [0]])
# Train the neural network on the XOR inputs and targets
for i in range(10000):
nn.train(inputs, targets, 0.1)
# Test the neural network on some inputs
print(nn.feedforward(np.array([0, 0])).round()) # should output [[0]]
print(nn feedforward(np array([0 1])) round()) # should output [[1]]
11/8/23, 3:01 AM ANN 8.ipynb - Colaboratory
https://colab.research.google.com/drive/16R-s_nE66KDK6wOkVYeyppi47i8BizK6#printMode=true 2/2
print(nn.feedforward(np.array([0, 1])).round()) # should output [[1]]
print(nn.feedforward(np.array([1, 0])).round()) # should output [[1]]
print(nn.feedforward(np.array([1, 1])).round()) # should output [[0]]
[0.]
[1.]
[1.]
[0.]